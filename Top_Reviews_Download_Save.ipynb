{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init\n",
    "To start have to run pip install for these packages. Once you run the next block you'll have to comment it out and run the rest of the document. Unfortunately, jupyter won't be able to find the command until you restart it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install google-play-scraper\n",
    "# This is for the sentiment analyzer. Since I used tensorflow, I have already installed it.\n",
    "# !pip install transformers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # https://medium.com/analytics-vidhya/google-play-store-apps-reviews-scraping-and-text-analytics-sentiment-analysis-5303294fffa7\n",
    "\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# from google_play_scraper import app, Sort, reviews_all, permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading reviews from playstore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getReviewDf(appId):\n",
    "    user_reviews = reviews_all(\n",
    "        appId,\n",
    "        sleep_milliseconds=0,\n",
    "        lang='en',\n",
    "        country='us',\n",
    "        sort=Sort.NEWEST\n",
    "    )\n",
    "    # convert to pandas pd\n",
    "    df_reviews = pd.DataFrame( np.array(user_reviews), columns=['review'])\n",
    "    df_reviews = df_reviews.join(pd.DataFrame(df_reviews.pop('review').tolist()))\n",
    "    return df_reviews   \n",
    "\n",
    "def getReviewsFrom(app_dict):\n",
    "    for app in app_dict.keys():\n",
    "        appid= app_dict[app]\n",
    "        user_reviews = reviews_all(\n",
    "            appid,\n",
    "            sleep_milliseconds=0,\n",
    "            lang='en',\n",
    "            country='us',\n",
    "            sort=Sort.NEWEST\n",
    "        )\n",
    "        # convert to pandas pd\n",
    "        df_reviews = pd.DataFrame( np.array(user_reviews), columns=['review'])\n",
    "        df_reviews = df_reviews.join(pd.DataFrame(df_reviews.pop('review').tolist()))\n",
    "        df_reviews.to_pickle(app)\n",
    "        return df_reviews\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "appsList = ['com.ChillyRoom.DungeonShooter',\n",
    " 'com.bandainamcoent.digimon_rearise_ww',\n",
    " 'com.sonypicturestelevision.WheelPuzzlePop',\n",
    " 'com.istomgames.engine',\n",
    " 'com.square_enix.android_googleplay.dq5',\n",
    " 'jp.co.ponos.battlecatstw',\n",
    " 'com.bandainamcoent.oprvww',\n",
    " 'com.tgc.sky.android',\n",
    " 'com.square_enix.android_googleplay.FFCCREww',\n",
    " 'com.square_enix.android_googleplay.subarashikikonosekai_solo',\n",
    " 'com.alpha.mpsen.android',\n",
    " 'com.superplanet.swordmaster',\n",
    " 'com.us.danmemo',\n",
    " 'com.quesera.kalpa',\n",
    " 'com.eyougame.xjhx.en',\n",
    " 'com.andromedagames.noblesseglobal',\n",
    " 'com.klab.utapri.shininglive.global']\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "import os.path\n",
    "from os import path\n",
    "\n",
    "from google_play_scraper import app, Sort, reviews_all, permissions\n",
    "\n",
    "for app_id in appsList:\n",
    "    \n",
    "    if path.exists('tempdfs/' + app_id):\n",
    "        continue\n",
    "    \n",
    "    appData = None\n",
    "    try:\n",
    "        # fetch the review data\n",
    "        temp_df = getReviewDf(app_id)\n",
    "\n",
    "        if temp_df is None:\n",
    "            continue\n",
    "            \n",
    "        # fetch the app information\n",
    "        appData = app(app_id, lang='en', country='us')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(app_id)\n",
    "    \n",
    "    if appData is None:\n",
    "        continue\n",
    "    \n",
    "    title = appData['title']\n",
    "#     display(title)\n",
    "    \n",
    "    temp_df['appId'] = appData['appId']\n",
    "    temp_df['updated'] = appData['updated']\n",
    "    temp_df['released'] = appData['released']\n",
    "#     reviews_df['appId'] = reviews_df['appId'].apply(lambda x: appData['appId'] if title in x else x)\n",
    "#     reviews_df['updated'] = reviews_df['updated'].apply(lambda x: appData['updated'] if title in x else x)\n",
    "#     reviews_df['released'] = reviews_df['released'].apply(lambda x: appData['released'] if title in x else x)\n",
    "\n",
    "    # we can clean up their timestamps while we are downloading\n",
    "    \n",
    "    from datetime import date,datetime\n",
    "    temp_df['updated'] = temp_df['updated'].apply(lambda x: datetime.fromtimestamp(x))\n",
    "    \n",
    "    #   # let's put some review dates here\n",
    "    # temp_df['review_posted_year'] = temp_df['at'].dt.year\n",
    "    # temp_df['review_posted_month'] = temp_df['at'].dt.month\n",
    "    # temp_df['review_posted_day'] = temp_df['at'].dt.day\n",
    "    # temp_df['review_posted_dayofweek'] = temp_df['at'].dt.dayofweek\n",
    "    # temp_df['review_posted_yearmonth'] = str(temp_df['at'].dt.year)+str(temp_df['at'].dt.month)\n",
    "\n",
    "    # temp_df['app_created_year'] = temp_df['released'].dt.year\n",
    "    # temp_df['app_lastupdated_year'] = temp_df['updated'].dt.year\n",
    "\n",
    "    # # to help with analysis we can base the reviews on year 0, year 1...etc\n",
    "    # temp_df['review_posted_year_norm'] = temp_df['review_posted_year'] - temp_df['app_created_year'] \n",
    "    # temp_df['review_posted_year_norm_months'] = temp_df['review_posted_year_norm'] +'-' + temp_df['review_posted_month'].str\n",
    "  \n",
    "    \n",
    "    temp_df.to_pickle('tempdfs/'+app_id)  \n",
    "    \n",
    "#     display(appData['appId'])\n",
    "#     display(appData['updated'])\n",
    "#     display(appData['released'])\n",
    "    \n",
    "#  'released', 'updated','appId','title'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load sentiment model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer,TFAutoModel, TFAutoModelWithLMHead\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import csv\n",
    "import urllib.request\n",
    "\n",
    "# # this is just a hack I tried to do to set the config directory.\n",
    "# # in the end I downloaded all of the files locally and didn't need this\n",
    "# %env SENTENCE_TRANSFORMERS_HOME=./.config\n",
    "# %env TRANSFORMERS_CACHE=./transformers\n",
    "\n",
    "\n",
    "MODEL = '/tf/school/UofM_School/Capstone/tweeteval_new/roberta-base-rt-sentiment'#tf_model.h5'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL, local_files_only=True)#'tf_model.h5')\n",
    "# tokenizer = AutoTokenizer.from_pretrained('./local_model_directory/')\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(MODEL)\n",
    "labels = ['negative','neutral','positive']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load from directory and perform sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load from directory and perform sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text):\n",
    "#     if text is None or text == '':\n",
    "#         return ''\n",
    "#     text = preprocess(text)\n",
    "#     if text is None or text =='':\n",
    "#         return\n",
    "    encoded_input = tokenizer(text, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
    "    return encoded_input\n",
    "    \n",
    "def GetReviewSentiment(encoded_input):\n",
    "    output = model(encoded_input)\n",
    "#     display(output)\n",
    "    scores = output[0][0].numpy()\n",
    "    scores = softmax(scores)\n",
    "\n",
    "    ranking = np.argsort(scores)\n",
    "    ranking = ranking[::-1]\n",
    "    return labels[ranking[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'tempdfs/'\n",
    "\n",
    "\n",
    "# get all files in the directory. Each file name should be the app-id that we used earlier\n",
    "from os import listdir, path\n",
    "from os.path import isfile, join\n",
    "onlyfiles = [f for f in listdir(mypath) if isfile(join(mypath, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentiment(text):\n",
    "        encoded_data = encode_text(list(text))\n",
    "        output = model(encoded_data)\n",
    "        scores = output[0].numpy()\n",
    "        scores = softmax(scores, axis=1)\n",
    "        ranking = np.argsort(scores,axis=1)\n",
    "        ranking = np.flip(ranking, axis=1)\n",
    "        sentiment = [labels[x[0]] for x in ranking]\n",
    "        return sentiment\n",
    "\n",
    "def batch_process_df(df, batch_size):\n",
    "    \n",
    "    \n",
    "    timeEstimates = {}\n",
    "    timeEstimates['count'] = 0\n",
    "    timeEstimates['value'] = 0\n",
    "    \n",
    "    import time\n",
    "\n",
    "#     start = time.time()\n",
    "#     print(\"hello\")\n",
    "#     end = time.time()\n",
    "#     print(end - start)\n",
    "\n",
    "    numBatches = int(len(df)/batch_size)\n",
    "\n",
    "    df['content'] = df['content'].apply(lambda x: 'NA' if x == None or x == 'None' else x)\n",
    "    df['sentiment'] = df['content']\n",
    "    \n",
    "    final_df = None\n",
    "    for i in range(0,numBatches):\n",
    "        \n",
    "        # show estimated time every 20 batches\n",
    "        \n",
    "        # we only want to show when the minute from last shown changes.\n",
    "        remainingTime = int(round((timeEstimates['value'] * (numBatches-i))/60, 0))\n",
    "        \n",
    "#         changedMinute = int(round(timeEstimates['value']/60, 0)) != remainingTime\n",
    "        if(timeEstimates['count'] > 0 and timeEstimates['count'] % 100 == 0):\n",
    "            print(f\"Estimated Time Remaining: {remainingTime} m\")\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        df_1 = df.iloc[i*batch_size: (i*batch_size) + batch_size,:]\n",
    "        # call the sentiment analizer on our batch\n",
    "        encoded_data = encode_text(list(df_1['content'].values))\n",
    "        \n",
    "        output = model(encoded_data)\n",
    "        scores = output[0].numpy()\n",
    "        scores = softmax(scores, axis=1)\n",
    "        ranking = np.argsort(scores,axis=1)\n",
    "        ranking = np.flip(ranking, axis=1)\n",
    "        sentiment = [labels[x[0]] for x in ranking]\n",
    "        \n",
    "        # attach the sentiment data to the df we are using\n",
    "        df_1['sentiment'] = pd.Series(sentiment).values\n",
    "        \n",
    "        \n",
    "        #  merge together into one big df\n",
    "        if final_df is None:\n",
    "            final_df = df_1\n",
    "        else:\n",
    "            final_df = pd.concat([final_df, df_1],ignore_index=True)\n",
    "            \n",
    "        end = time.time()\n",
    "        \n",
    "        # let's average out the timer\n",
    "        executionTime = end - start\n",
    "        \n",
    "#         print(end - start)\n",
    "        \n",
    "        if(timeEstimates['count']== 0):\n",
    "            timeEstimates['count'] = 1\n",
    "            timeEstimates['value'] = executionTime\n",
    "        else:\n",
    "            # undo the averaging we did and append this.\n",
    "            oldTotal = timeEstimates['value'] * timeEstimates['count']\n",
    "            \n",
    "            timeEstimates['count'] = timeEstimates['count'] + 1\n",
    "            timeEstimates['value'] = (oldTotal + executionTime)/timeEstimates['count']\n",
    "        \n",
    "            \n",
    "    if len(df) % batch_size != 0:\n",
    "        # we have a few remaining items to process\n",
    "        df_1 = df.iloc[numBatches*batch_size:,:]\n",
    "        # call the sentiment analizer on our batch\n",
    "        encoded_data = encode_text(list(df_1['content'].values))\n",
    "        \n",
    "        output = model(encoded_data)\n",
    "        scores = output[0].numpy()\n",
    "        scores = softmax(scores, axis=1)\n",
    "        ranking = np.argsort(scores,axis=1)\n",
    "        ranking = np.flip(ranking, axis=1)\n",
    "        sentiment = [labels[x[0]] for x in ranking]\n",
    "        \n",
    "        # attach the sentiment data to the df we are using\n",
    "        df_1['sentiment'] = pd.Series(sentiment).values\n",
    "        final_df = pd.concat([final_df, df_1],ignore_index=True)\n",
    "#         The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
    "        \n",
    "    return final_df\n",
    "\n",
    "def WorkerJob(onlyfiles, threadNum):\n",
    "    for app_id in onlyfiles:\n",
    "        print(f\"thread {threadNum}: loading file: {app_id}\")\n",
    "        temp_df = pd.read_pickle(mypath + app_id)\n",
    "        \n",
    "        print(f\"thread {threadNum}: encoding text for analysis...\")\n",
    "#         temp_df['content'] = temp_df['content'].apply(lambda x: x if x !='None' or x != None else ' None ')\n",
    "#         display(temp_df['content'].values[:100])\n",
    "        temp_df['content'] = temp_df['content'].apply(lambda x: 'NA' if x == None or x == 'None' else x)\n",
    "#         encoded_data = encode_text(list(temp_df['content'].values))\n",
    "        \n",
    "        print(f\"thread {threadNum}: performing sentiment analysis on: {temp_df['title'].unique()[0]}\")\n",
    "        temp_df = batch_process_df(temp_df, 16)\n",
    "#      Try using .loc[row_indexer,col_indexer] = value instead   \n",
    "        \n",
    "        \n",
    "        print(f\"thread {threadNum}: Done with {app_id}\")\n",
    "        temp_df.to_pickle(mypath + app_id)\n",
    "        \n",
    "WorkerJob(onlyfiles, 'only worker')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
